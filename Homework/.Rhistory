# principal component regression
# Build the model on training set
set.seed(1)
pcr <- train(
crim~., data = Boston_train, method = "pcr",
scale = TRUE,
trControl = trainControl("cv", number = 12),
tuneLength = 10
)
# Plot model RMSE vs different values of components
plot(pcr)
# Print the best tuning parameter ncomp that
# minimize the cross-validation error, RMSE
pcr$bestTune
# Summarize the final model
summary(pcr$finalModel)
# Make predictions
predictions <- pcr %>% predict(Boston_test)
# Model performance metrics
data.frame(
RMSE = caret::RMSE(predictions, Boston_test$crim),
Rsquare = caret::R2(predictions, Boston_test$crim)
)
# principal component regression
# Build the model on training set
set.seed(1)
pcr <- train(
crim~., data = Boston_train, method = "pcr",
scale = TRUE,
trControl = trainControl("cv", number = 10),
tuneLength = 10
)
# Plot model RMSE vs different values of components
plot(pcr)
# Print the best tuning parameter ncomp that
# minimize the cross-validation error, RMSE
pcr$bestTune
# Summarize the final model
summary(pcr$finalModel)
# Make predictions
predictions <- pcr %>% predict(Boston_test)
# Model performance metrics
data.frame(
RMSE = caret::RMSE(predictions, Boston_test$crim),
Rsquare = caret::R2(predictions, Boston_test$crim)
)
# principal component regression
# Build the model on training set
set.seed(1)
pcr <- train(
crim~., data = Boston_train, method = "pcr",
scale = TRUE,
trControl = trainControl("cv", number = 5),
tuneLength = 10
)
# Plot model RMSE vs different values of components
plot(pcr)
# Print the best tuning parameter ncomp that
# minimize the cross-validation error, RMSE
pcr$bestTune
# Summarize the final model
summary(pcr$finalModel)
# Make predictions
predictions <- pcr %>% predict(Boston_test)
# Model performance metrics
data.frame(
RMSE = caret::RMSE(predictions, Boston_test$crim),
Rsquare = caret::R2(predictions, Boston_test$crim)
)
# principal component regression
# Build the model on training set
set.seed(1)
pcr <- train(
crim~., data = Boston_train, method = "pcr",
scale = TRUE,
trControl = trainControl("cv", number = 10),
tuneLength = 10
)
# Plot model RMSE vs different values of components
plot(pcr)
# Print the best tuning parameter ncomp that
# minimize the cross-validation error, RMSE
pcr$bestTune
# Summarize the final model
summary(pcr$finalModel)
# Make predictions
predictions <- pcr %>% predict(Boston_test)
# Model performance metrics
data.frame(
RMSE = caret::RMSE(predictions, Boston_test$crim),
Rsquare = caret::R2(predictions, Boston_test$crim)
)
# principal component regression
# Build the model on training set
set.seed(1)
pcr <- train(
crim~., data = Boston_train, method = "pcr",
scale = TRUE,
trControl = trainControl("cv", number = 10),
tuneLength = 12
)
# Plot model RMSE vs different values of components
plot(pcr)
# Print the best tuning parameter ncomp that
# minimize the cross-validation error, RMSE
pcr$bestTune
# Summarize the final model
summary(pcr$finalModel)
# Make predictions
predictions <- pcr %>% predict(Boston_test)
# Model performance metrics
data.frame(
RMSE = caret::RMSE(predictions, Boston_test$crim),
Rsquare = caret::R2(predictions, Boston_test$crim)
)
# principal component regression
# Build the model on training set
set.seed(1)
pcr <- train(
crim~., data = Boston_train, method = "pcr",
scale = TRUE,
trControl = trainControl("cv", number = 10),
tuneLength = 10
)
# Plot model RMSE vs different values of components
plot(pcr)
# Print the best tuning parameter ncomp that
# minimize the cross-validation error, RMSE
pcr$bestTune
# Summarize the final model
summary(pcr$finalModel)
# Make predictions
predictions <- pcr %>% predict(Boston_test)
# Model performance metrics
data.frame(
RMSE = caret::RMSE(predictions, Boston_test$crim),
Rsquare = caret::R2(predictions, Boston_test$crim)
)
# principal component regression
# Build the model on training set
set.seed(1)
pcr <- train(
crim~., data = Boston_train, method = "pcr",
scale = TRUE,
trControl = trainControl("cv", number = 10),
tuneLength = 13
)
# Plot model RMSE vs different values of components
plot(pcr)
# Print the best tuning parameter ncomp that
# minimize the cross-validation error, RMSE
pcr$bestTune
# Summarize the final model
summary(pcr$finalModel)
# Make predictions
predictions <- pcr %>% predict(Boston_test)
# Model performance metrics
data.frame(
RMSE = caret::RMSE(predictions, Boston_test$crim),
Rsquare = caret::R2(predictions, Boston_test$crim)
)
library(tidyverse)
library(glmnet)
library(caret)
library(pls)
library(ISLR2)
# prepare data
head(College)
College <- na.omit(College)
rownames(College) <- College$X
College = College[,-1] # remove original College names column
set.seed(1)
# split data into testing and training sets:
train_size = dim(College)[1] / 2 #  half the number of rows in the College set
train = sample(1:dim(College)[1], train_size) # training sample indices
test = -train # testing sample indices ('-' means 'exclude')
# create training set from subset of College data set at the training indices
College_train = College[train, ]
# create testing set from subset of College data set at the testing indices
College_test = College[test, ]
lm_fit = lm(Apps~., data=College_train) # fit lm
lm_pred = predict(lm_fit, College_test) # predict test using lm fit
mean((College_test[, "Apps"] - lm_pred)^2) # test mean squared error
train_matrix <- model.matrix(Apps ~ ., data = College_train) # generate training matrix
test_matrix <- model.matrix(Apps ~ ., data = College_test) # generate testing matrix
grid <- 10 ^ seq(10, -2, length = 100) # values of lambda to try ridge regression with
# fit ridge regression models using values of lambda specified in grid
ridge_fit <- glmnet(train_matrix, College_train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
# cross validate the regression models cv.glmnet
cv_ridge <- cv.glmnet(train_matrix, College_train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
# get the best value of lambda from the results of the cross validation
bestlam_ridge <- cv_ridge$lambda.min
bestlam_ridge
# using the best value of lambda, predict values in the test set using the
# corresponding model in ridge_fit
# note: this uses the predict function from the glmnet library, not the base r one
pred_ridge <- predict(ridge_fit, s = bestlam_ridge, newx = test_matrix)
mean((pred_ridge - College_test$Apps)^2) # mean squared error
# get coefficient estimates
predict(ridge_fit, s = bestlam_ridge, type = "coefficients")
# alternative using caret with 10-fold cross validation
# instructions found at http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/153-penalized-regression-essentials-ridge-lasso-elastic-net/#using-caret-package
set.seed(1)
ridge <- train(Apps~., data = College_train,
method = "glmnet",
trControl = trainControl(
method = "cv", number = 10,
verboseIter = F
),
tuneGrid = expand.grid(alpha = 0, lambda = grid))
# Model coefficients
coef(ridge$finalModel, ridge$bestTune$lambda)
# Make predictions
predictions <- ridge %>% predict(College_test)
# Model prediction performance
# return selected model metrics
data.frame(
RMSE = RMSE(predictions, College_test$Apps),
Rsquare = caret::R2(predictions, College_test$Apps)
)
# same procedure as for ridge regression
lasso_fit <- glmnet(train_matrix, College_train$Apps, alpha = 1, lambda = grid, thresh = 1e-12)
cv_lasso <- cv.glmnet(train_matrix, College_train$Apps, alpha = 1, lambda = grid, thresh = 1e-12)
bestlam_lasso <- cv_lasso$lambda.min
bestlam_lasso
pred_lasso <- predict(lasso_fit, s = bestlam_lasso, newx = test_matrix)
mean((pred_lasso - College_test$Apps)^2)
# get coefficient estimates:
predict(lasso_fit, s = bestlam_lasso, type = "coefficients")
# alternative using caret with 10-fold cross validation
set.seed(1)
lasso <- train(Apps~., data = College_train,
method = "glmnet",
trControl = trainControl(
method = "cv", number = 10,
verboseIter = F
),
tuneGrid = expand.grid(alpha = 1, lambda = grid))
# Model coefficients
coef(lasso$finalModel, ridge$bestTune$lambda)
# Make predictions
predictions <- lasso %>% predict(College_test)
# Model prediction performance
# return selected model metrics
data.frame(
RMSE = RMSE(predictions, College_test$Apps),
Rsquare = caret::R2(predictions, College_test$Apps)
)
pcr_fit <- pcr(Apps ~ ., data = College_train, scale = TRUE, validation = "CV")
validationplot(pcr_fit, val.type = "MSEP")
# summary(pcr_fit)
pcr_pred <- predict(pcr_fit, College_test, ncomp = 10)
# ncomp could also be lower if the number of components is a concern
mean((pcr_pred - College_test$Apps)^2)
pcr_fit <- pcr(Apps ~ ., data = College, scale = TRUE, ncomp = 10)
summary(pcr_fit)
# alternative using caret with 10-fold cross validation
# solution found at http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/152-principal-component-and-partial-least-squares-regression-essentials/#principal-component-regression
# Build the model on training set
set.seed(1)
pcr <- train(
Apps~., data = College_train, method = "pcr",
scale = TRUE,
trControl = trainControl("cv", number = 10),
tuneLength = 10
)
# Plot model RMSE vs different values of components
plot(pcr)
# Print the best tuning parameter ncomp that
# minimize the cross-validation error, RMSE
pcr$bestTune
# Summarize the final model
summary(pcr$finalModel)
# Make predictions
predictions <- pcr %>% predict(College_test)
# Model performance metrics
data.frame(
RMSE = caret::RMSE(predictions, College_test$Apps),
Rsquare = caret::R2(predictions, College_test$Apps)
)
pls_fit <- plsr(Apps ~ ., data = College_train, scale = TRUE, validation = "CV")
validationplot(pls_fit, val.type = "MSEP")
pls_pred <- predict(pls_fit, College_test, ncomp = 4)
mean((pls_pred - College_test$Apps)^2)
# alternative using caret with 10-fold cross validation
# Build the model on training set
set.seed(1)
pls <- train(
Apps~., data = College_train, method = "pls",
scale = TRUE,
trControl = trainControl("cv", number = 10),
tuneLength = 10
)
# Plot model RMSE vs different values of components
plot(pls)
# Print the best tuning parameter ncomp that
# minimize the cross-validation error, RMSE
pls$bestTune
# Summarize the final model
summary(pls$finalModel)
# Make predictions
predictions <- pls %>% predict(College_test)
# Model performance metrics
data.frame(
RMSE = caret::RMSE(predictions, College_test$Apps),
Rsquare = caret::R2(predictions, College_test$Apps)
)
Boston <- na.omit(Boston)
head(Boston)
set.seed(1)
# split data into testing and training sets:
train_size = dim(Boston)[1] / 2
train = sample(1:dim(Boston)[1], train_size)
test = -train
Boston_train = Boston[train, ]
Boston_test = Boston[test, ]
# best subset selection (exhaustive)
regfit_full <- regsubsets(crim ~ ., data = Boston_train, nvmax = 12)
reg_summary <- summary(regfit_full)
reg_summary
plot_metrics <- data.frame(rss = reg_summary$rss, adjr2 = reg_summary$adjr2, numvar = 1:12)
plot_metrics %>% filter(adjr2 == max(adjr2))
plot_metrics %>%
ggplot(aes(y = rss, x = numvar)) +
geom_point() +
geom_line() +
xlab("Number of Variables") + ylab("RSS") +
theme_bw() +
ggtitle("RSS for addition of each variable")
plot_metrics %>%
ggplot(aes(y = adjr2, x = numvar)) +
geom_point() +
geom_line() +
xlab("Number of Variables") + ylab("Adjusted R2") +
theme_bw() +
ggtitle("Adjusted R2 for addition of each variable")
grid <- 10 ^ seq(10, -2, length = 100) # values of lambda to try
# ridge regression
ridge <- train(crim~., data = Boston_train,
method = "glmnet",
trControl = trainControl(
method = "cv", number = 10,
verboseIter = F
),
tuneGrid = expand.grid(alpha = 0, lambda = grid))
# suggested lambda
ridge$bestTune$lambda
# Model coefficients
coef(ridge$finalModel, ridge$bestTune$lambda)
# Make predictions
predictions <- ridge %>% predict(Boston_test)
# Model prediction performance
# return selected model metrics
data.frame(
RMSE = RMSE(predictions, Boston_test$crim),
Rsquare = caret::R2(predictions, Boston_test$crim)
)
# lasso regression
lasso <- train(crim~., data = Boston_train,
method = "glmnet",
trControl = trainControl(
method = "cv", number = 10,
verboseIter = F
),
tuneGrid = expand.grid(alpha = 1, lambda = grid))
# suggested lambda
lasso$bestTune$lambda
# Model coefficients
coef(lasso$finalModel, ridge$bestTune$lambda)
# Make predictions
predictions <- lasso %>% predict(Boston_test)
# Model prediction performance
# return selected model metrics
data.frame(
RMSE = RMSE(predictions, Boston_test$crim),
Rsquare = caret::R2(predictions, Boston_test$crim)
)
# principal component regression
# Build the model on training set
set.seed(1)
pcr <- train(
crim~., data = Boston_train, method = "pcr",
scale = TRUE,
trControl = trainControl("cv", number = 10),
tuneLength = 13
)
# Plot model RMSE vs different values of components
plot(pcr)
# Print the best tuning parameter ncomp that
# minimize the cross-validation error, RMSE
pcr$bestTune
# Summarize the final model
summary(pcr$finalModel)
# Make predictions
predictions <- pcr %>% predict(Boston_test)
# Model performance metrics
data.frame(
RMSE = caret::RMSE(predictions, Boston_test$crim),
Rsquare = caret::R2(predictions, Boston_test$crim)
)
sessionInfo()
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
# set your initial condition and desired number of iterations:
x_0s <- c(3.43, 4.43, 7)
N <- 100
# set the iteration plot x axis range (lower and upper bounds):
x_min <- 0; x_max <- 8
y_min <- -2; y_max <- 8
use_custom_range_x <- FALSE
use_custom_range_y <- FALSE
# declare your function here:
func <- function(x){
return(-2 * sin(x) + x) # function goes here
}
get_function_data <- function(range = c(-1, 1), steps = 100){
steps_multiplier <- (range[2]-range[1])/10
if(steps_multiplier < 1){steps_multiplier <- 1}
# adds steps to get data for depending on the number of 10s
# in the specified plot x range
x <- seq(from = range[1], to = range[2], length.out = steps * steps_multiplier)
y <- array(dim = steps * steps_multiplier)
for(i in 1:length(x)){
y[i] <- func(x[i])
}
return(data.frame(x = x, y = y))
}
graphical_iterator <- function(x_0s, N = 100){
segments <- data.frame()
for(i in x_0s){
start <- i
vert <- FALSE
x_0 <- rep(i,times=1+(N*2))
xstarts <- c(start)
ystarts <- c(y_min)
xends <- c(start)
yends <- c(func(start))
# iteratively get the coordinates of the next segment points
for(i in 1:(2 * N))
# range = 2 * N because every step will be described by two segments
{
# if the last segment was vertical, the next must be horizontal
if(vert){
xstarts <- c(xstarts, start)
ystarts <- c(ystarts, start)
xends <- c(xends, start)
yends <- c(yends, func(start))
vert <- FALSE
}
else{
xstarts <- c(xstarts, start)
ystarts <- c(ystarts, func(start))
xends <- c(xends, func(start))
yends <- c(yends, func(start))
vert <- TRUE
start <- func(start) # update start value
}
}
segments <- rbind(segments, data.frame(x_0s = x_0, xstarts, ystarts, xends, yends))
}
return(segments)
}
cobweb_trajects <- graphical_iterator(x_0s = x_0s, N = N)
if(use_custom_range_x == FALSE){
x_min <- min(cobweb_trajects$xstarts); x_max <- max(cobweb_trajects$xends)
}
if(use_custom_range_y == FALSE){
y_min <- min(cobweb_trajects$xstarts); y_max <- max(cobweb_trajects$xends)
}
plot_data <- get_function_data(range = c(x_min,x_max)) # gets the plotting data
get_function_iteration_trajectories <- function(x_0s, N = 100){
trajectories <- data.frame()
for(i in x_0s){
x_t <- i
x_0 <- rep(i,times=N+1)
n <- 0:N
trajectory <- c(x_t)
for(t in 0:(N-1)){
x_t <- func(x_t)
trajectory <- c(trajectory, x_t) # add x_t_1's value to the trajectory vector
}
trajectories <- rbind(trajectories, data.frame(x_0s = x_0, ns = n, trajectories = trajectory))
}
return(trajectories)
}
trajectories <- get_function_iteration_trajectories(x_0s = x_0s, N = N)
plot_data %>%
ggplot(aes(x, y)) +
geom_line(colour = "black") +
geom_abline(linetype = "dashed") +
geom_segment(data = cobweb_trajects, aes(x = xstarts, y = ystarts, xend = xends,
yend = yends, colour=as.factor(x_0s))) +
coord_cartesian(xlim = c(x_min, x_max), ylim = c(y_min, y_max))
trajectories
# trajectory plot
trajectories %>%
ggplot(aes(ns, trajectories, colour = as.factor(x_0s))) +
geom_line() + labs(x="n")
# TODO
# average distances between trajectories plot
# mean(dist(c(1:10))) # mean of distances between whole numbers from 1 to 10 (example)
# trajectories$x_0s <- paste0('x_0_', trajectories$x_0s)
# trajectories_wide <- trajectories %>%
#   pivot_wider(names_from = x_0s, values_from = trajectories) %>%
#   mutate(distance = (x_0_2.01-x_0_2))
#
# trajectories_wide
#
# trajectories_wide %>%
#   ggplot(aes(ns, distance)) +
#   geom_point() + geom_line() +
#   labs(x="n", y="distance between trajectories")
